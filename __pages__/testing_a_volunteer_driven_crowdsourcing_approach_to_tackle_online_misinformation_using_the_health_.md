# Testing a volunteer-driven crowdsourcing approach to tackle online misinformation using the Health…

[[focal_point:lillian.njoro@undp.org]]

[[year:2022]]

[[type:experiment]]

[[sdgs:9. Industry, innovation and infrastructure]]
[[sdgs:16. Peace, justice and strong institutions]]
[[sdgs:17. Partnerships for the goals]]
[[country:Kenya]]
[[latlng:0.5507660771276834,37.82243990624841]]
[[assignment_type:Random assignment]]
[[control_group:It does not use a control group]]
[[quality_check:The hypothesis is clearly stated]]
[[quality_check:This activity has a low risk]]
[[quality_check:This activity is relevant to a CPD outcome]]
[[quality_check:This activity offers strong collaboration oportunities]]
[[sample_size:100-999]]
[[total_cost:More than 20,000 USD]]
## Overview
**This section is to explain the basics of the activity**

> Name of the experimental activity

> Prepared by (Name of the experimenter)

> On date (Day/Month/Year)

> Current status of experimental activity

> What portfolio does this activity correspond to? If any

> What is the frontier challenge does this activity responds to?

> What is the learning question(from your action learning plan) is this activity related to?

> Please categorize the type that best identifies this experimental activity:

- 

> Which sector are you partnering with for this activity? Please select all that apply

- 

> Please list the names of partners mentioned in the previous question:
## Design
**Explain the design of the experimental activity. In general, experimental activities consist on trying to learn how results are connected to a stimuli.**

> What is the specific learning intent of the activity?

Testing the value proposition of a volunteer-driven crowdsourcing approach to tackling online misinformation using the Healthy Internet Project (HIP) platform

> What is your hypothesis? IF... THEN....

If we can understand the behavioural motivations towards citizen engagement on online mis/disinformation then we can strengthen interventions that leverage the volunteer-driven crowdsourcing approach.

> Does the activity use a control group for comparison?

- No, it does not use a control group

> How is the intervention assigned to different groups in your experiment?

- Random assignment

> Describe which actions will you take to test your hypothesis:

Finalise partnership agreement with Busara Centre (the knowledge partner), refine the scope of the experiment, recruit the volunteers for the natural experiment, monitor and analyse the data collected, follow up with in-depth interviews and focus group discussions, develop insights and recommendations, draft and finalise the experiment report, disseminate the results widely

> What is the unit of analysis of this experimental activity?

Number of volunteers/ participants

> Please describe the data collection technique proposed

1) Data on the flagged online content from the back-end of the HIP platform 2) Qualitative data from the in-depth interviews and focus group discussions of a sample of the participants

> What is the timeline of the experimental activity? (Months/Days)

4 months

> What is the estimated sample size?

- 100-999

> What is the total estimated monetary resources needed for this experiment?

> Quality Check

- This activity is relevant to a CPD outcome
- The hypothesis is clearly stated
- This activity offers strong collaboration oportunities
- This activity has a low risk

> Please upload any supporting images or visuals for this experiment.


![Missing alt text](https://undp-accelerator-labs.github.io/Archive_SDG_Commons_2022/blobs/experiments/7b404f34e75c3316160d9ffd5a896cff.jpg)


> Please upload any supporting links
[https://undp-kenya.medium.com/two-truths-and-a-lie-an-experiment-to-evaluate-behaviours-in-identifying-online-misinformation-6cc731d6dd88](https://undp-kenya.medium.com/two-truths-and-a-lie-an-experiment-to-evaluate-behaviours-in-identifying-online-misinformation-6cc731d6dd88)

> What are the estimated non- monetary resources required for this experiment? (time allocation from team, external resources, etc) If any.

n/a
## Results
**Only complete this section when presenting results**

> Was the original hypothesis (If.. then) proven or disproven?

Proven

> Do you have observations about the methodology chosen for the experiment? What would you change?

The methodology chosen for the experiment involving the live experimental demonstration and mixed methods behavioural mapping phase was sufficient to respond to the learning questions. What could have been changed would be to include user segmentation to analyse behaviours of different clusters of users for more in-depth analysis

> From design to results, how long did this activity take? (Time in months)

5 months

> What were the actual monetary resources invested in this activity? (Amount in USD)

24,622

> Does this activity have a follow up or a next stage? Please explain

The Acc Lab in partnership with Busara hosted a dissemination webinar in April 2022 to share the findings with stakeholders in the fact-checking and media ecosystem. 

> Is this experiment planned to scale? How? With whom?

The Acc Lab is working with the Deepening Democracy Programme under the Governance portfolio to explore the possibility of scaling the insights into ongoing elections-related interventions such as the iVerify fact-checking project which was piloted by UNDP in Zambia last year.

> Please include any supporting images that could be used to showcase this activity


![Missing alt text](https://undp-accelerator-labs.github.io/Archive_SDG_Commons_2022/blobs/experiments/f33b6dc6fbe867f14a2d75865c36a86f.jpg)


> Please add any supporting links that describe the planning, implementation, results of learning of this activity? For example a tweet, a blog, or a report. 

1) https://www.ke.undp.org/content/kenya/en/home/library/undp-reports/evaluating-crowdsourcing-behaviours-in-identifying-online-misinf.html 2) https://undp-kenya.medium.com/two-truths-and-a-lie-an-experiment-to-evaluate-behaviours-in-identifying-online-misinformation-6cc731d6dd88;   3) https://www.youtube.com/watch?v=ildRvXza_ys&ab_channel=BusaraCenterforBehavioralEconomics 
## Learning
**This section is aimed at presenting the learning outcomes from this activity. **

> 

Learning

> What do you know now about the action plan learning question that you did not know before? What were your main learnings during this experiment?

1) The pilot experiment was based on exploring the relevance and usability of the Healthy Internet Project (HIP) tool to report potential mis/disinformation online within a Kenyan context. Overall, while the respondents deemed that HIP was a useful tool the experiment revealed that there was need for further improvements to the tool; for example integrating a mobile phone interface rather than being strictly for PC users; and having a stronger feedback mechanism to close the loop with the users.  2) Anonymity was a key concern when it came to flagging mis/disinformation with respondents citing fear of potential backlash especially when reporting politically-related content. It's therefore imperative that any reporting platforms/ tools include clear messaging to ensure users of their anonymity and protection from any risks associated with reporting harmful content.  3) There are significant gaps in the general public's understanding of mis/disinformation. Upon conducting focus group discussions and in-depth interviews, it was revealed that majority of the users were flagging content based on negative sentiments such as dislike of a topic, and there was an emotional element attached. The respondents also shared that they mostly determine what is trustworthy content online based on their own intuition/ gut feeling or by relying on what they consider 'trusted' sources such as certain family members or major news publications websites. More awareness creation is required to ensure that the public are adequately informed on how to spot and avoid falling prey to online mis/disinformation and what channels to take if they come across such content while navigating the multitude of online platforms on the web.  4) In order to make a volunteer-driven crowdsourcing approach valuable for fact-checkers, it's important to sensitise users on what mis/disinformation looks like to avoid sentiment-based reporting. Additionally, flagging tools like HIP should ensure that the users post specific content that requires verification, such as phrases, rather than whole articles or websites, to enable the flagged content to be more valuable to fact-checking organisations.  5) In terms of motivations of respondents to flag content, one of the strongest incentives that came up was the need to provide a stronger feedback mechanism to communicate back to the user what happened with the content they flagged. This helps to ensure that users feel that their actions are actually making a difference and feel motivated to keep going. Some respondents raised the issue of monetary compensation to continue flagging content but this raises issues of long-term sustainability of such efforts as well as creating opportunities for nefarious forces to corrupt the flagging process. Related to the monetary compensation, another suggestion that came up from the fact-checking organisations was to include minor appreciations like data bundles to users whose flagged content results in some sort of follow-up action but again the issue of sustainability comes in. It's important to tap into non-financial motivations for such approaches to facilitate long term sustainability of these kinds of interventions.

> What were the main obstacles and challenges you encountered during this activity?

This was the Acc Lab's first time using the Responsible Party Agreement and it took some time to figure out how to navigate that process with the Programme Management Support Unit colleagues.

> Who at UNDP might benefit from the results of this experimental activity? Why?

Programmes under the Governance portfolio, specifically the Deepening Democracy project

> Who outside UNDP might benefit from the results of this experiment? and why?

Fact-checking organisations like PesaCheck

> Did this experiment require iterations? If so, how many and what did you change/adjust along the way? and why?

There were no iterations

> What advice would you give someone wanting to replicate this experimental activity?

To consider including the user segmentation and to bring onboard a fact-checking partner early on in the process to support in reviewing the accuracy of the flagged content

> Can this experiment be replicated in another thematic area or other SDGs? If yes, what would need to be considered, if no, why not?

Yes. The participants would need to be clearly instructed during the onboarding phase on the scope of the experiment so as to not go beyond that intended scope

> How much the "sense" and "explore" phases of the learning cycle influenced/shaped this experiment? In hindsight, what would you have done differently with your fellow Solution Mapper and Explorer?

Not as much since the experiment was building on a previous portfolio on COVID-19 misinformation

> What surprised you?

Not a surprise, but rather an appreciation of the value add of doing an experiment with a strong knowledge partner like Busara
