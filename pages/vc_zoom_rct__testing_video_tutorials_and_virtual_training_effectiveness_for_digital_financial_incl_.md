# VC Zoom RCT: testing video tutorials and virtual training effectiveness for digital financial incl…

[[focal_point:victor.tablas@undp.org]]

[[year:2022]]

[[type:experiment]]

[[sdgs:5. Gender equality]]
[[sdgs:8. Decent work and economic growth]]
[[sdgs:10. Reduced innequalities]]
[[country:El Salvador]]
[[latlng:13.735849549208535,-88.87103252208958]]
[[assignment_type:Random assignment]]
[[control_group:Yes, a different group entirely]]
[[control_group:Yes, the same group but before the intervention]]
[[quality_check:The hypothesis is clearly stated]]
[[quality_check:This activity has a low risk]]
[[quality_check:This activity is relevant to a CPD outcome]]
[[quality_check:This activity offers a high potential for scaling]]
[[quality_check:This activity offers strong collaboration oportunities]]
[[sample_size:50-99]]
[[total_cost:Less than 1,000 USD]]
## Overview
**This section is to explain the basics of the activity**

> Name of the experimental activity

> Prepared by (Name of the experimenter)

> On date (Day/Month/Year)

> Current status of experimental activity

> What portfolio does this activity correspond to? If any

> What is the frontier challenge does this activity responds to?

> What is the learning question(from your action learning plan) is this activity related to?

> Please categorize the type that best identifies this experimental activity:

- 

> Which sector are you partnering with for this activity? Please select all that apply

- 

> Please list the names of partners mentioned in the previous question:
## Design
**Explain the design of the experimental activity. In general, experimental activities consist on trying to learn how results are connected to a stimuli.**

> What is the specific learning intent of the activity?

Our challenge is to understand the role that informal and formal digital support systems may play in bridging the digital divide with an emphasis on populations that are being left behind in the new digital economy, causing the already existing inequalities (gender, socioeconomic status) to exacerbate.

> What is your hypothesis? IF... THEN....

Hypothesis C1: If business owners are exposed to short video tutorials on digital financial services, then they report a trust increase in such services.   Hypothesis C2: If business owners are exposed to short video tutorials on digital financial services, then they report a knowledge increase about these services.   Hypothesis C3: If business owners are exposed to short video tutorials and virtual training on digital financial services, then they report a trust increase in such services.  Hypothesis C4: If business owners are exposed to short video tutorials and virtual training on digital financial services, then they report a knowledge increase about these services.

> Does the activity use a control group for comparison?

- Yes, a different group entirely
- Yes, the same group but before the intervention

> How is the intervention assigned to different groups in your experiment?

- Random assignment

> Describe which actions will you take to test your hypothesis:

Phase-in Randomized Control Trial through VC Zoom to test the effectiveness of video tutorials and virtual training in developing trust and self-reported skills in financial digital services. Using an RCT will provide solid evidence about one of the most difficult topics in terms of digital inclusion (financial services) and its potential to scale-up these tools.

> What is the unit of analysis of this experimental activity?

Entrepreneurs participating in the Digi-Chiquihuites Project that regularly attend Zoom sessions of the module on financial tools.

> Please describe the data collection technique proposed

VC Zoom Surveys

> What is the timeline of the experimental activity? (Months/Days)

2 months

> What is the estimated sample size?

- 50-99

> What is the total estimated monetary resources needed for this experiment?

> Quality Check

- This activity is relevant to a CPD outcome
- The hypothesis is clearly stated
- This activity offers strong collaboration oportunities
- This activity offers a high potential for scaling
- This activity has a low risk

> Please upload any supporting images or visuals for this experiment.


![Missing alt text](https://undp-accelerator-labs.github.io/Archive_SDG_Commons_2022/blobs/experiments/d4d5d8629fd7a95631a8f5c4f7fefed6.png)


> Please upload any supporting links

[https://www.sv.undp.org/content/el_salvador/es/home/presscenter/articles/2021/011111/canasta-digital-para-promover-medios-de-vida--proteccion-social-.html https://www.youtube.com/watch?v=bxmt6NQu6nw https://diarioelsalvador.com/pnud-mitur-e-istu-promueven-inclusion-digital-y-financiera-en-el-sector-turismo/169099/](https://www.sv.undp.org/content/el_salvador/es/home/presscenter/articles/2021/011111/canasta-digital-para-promover-medios-de-vida--proteccion-social-.html https://www.youtube.com/watch?v=bxmt6NQu6nw https://diarioelsalvador.com/pnud-mitur-e-istu-promueven-inclusion-digital-y-financiera-en-el-sector-turismo/169099/)

> What are the estimated non- monetary resources required for this experiment? (time allocation from team, external resources, etc) If any.

External resources: CONEXION an NGO that developed and delivered the training program to project participants. UNDP Internal resources: Digi-Chiquihuites Project team, UNDP Accelerator ELSV Lab team.
## Results
**Only complete this section when presenting results**

> Was the original hypothesis (If.. then) proven or disproven?

Hypothesis C1 was rejected; C2, C3 and C4 were not rejected.

> Do you have observations about the methodology chosen for the experiment? What would you change?

Something unforeseen was that since the experiment would take place through Zoom, participants would type in their own names when joining the calls. In a few cases they just wrote a first name or a nick name, which led to difficulties cleaning the datasets collected from the Zoom polls and linking results back to the baseline. For a future experiment actions should be taken to overcome this challenge.  

> From design to results, how long did this activity take? (Time in months)

2 months

> What were the actual monetary resources invested in this activity? (Amount in USD)

The resources used were the following: a 3 minutes video tutorial developed by CONEXION and a 12 minutes training delivered by two instructors. Also a raffle was included at the end of Zoom session to encourage attendance. Approximately, each video tutorial costs on average $255.00 and each training session costs around $600, 20 raffled items are estimated at $100 in total, which adds up to a total of USD$955. Experiment design was conducted in-house. These funds were not disbursed by the Lab, but from the Digi-Chiquihuites project, that hired CONEXION.

> Does this activity have a follow up or a next stage? Please explain

Yes, there is a new project at UNDP that will implement a new digital baskets set for SMEs, through Korean Cooperation funds (KOICA).

> Is this experiment planned to scale? How? With whom?

Yes, recently UNDP and KOICA signed in 2022 a project to foster the development of dynamic micro and small businesses for economic reactivation of El Salvador through the National Commission for Micro and Small Enterprises (CONAMYPE). This project will use the learnings from Digi-Chiquihuites to design and deliver over 150 new digital baskets for larger businesses.

> Please include any supporting images that could be used to showcase this activity


![Missing alt text](https://undp-accelerator-labs.github.io/Archive_SDG_Commons_2022/blobs/experiments/76f6fdc1ea33081fc47cf6c48028779f.png)


> Please add any supporting links that describe the planning, implementation, results of learning of this activity? For example a tweet, a blog, or a report. 
[https://twitter.com/PNUDSV/status/1395505956105203712](https://twitter.com/PNUDSV/status/1395505956105203712)
## Learning
**This section is aimed at presenting the learning outcomes from this activity. **

> What do you know now about the action plan learning question that you did not know before? What were your main learnings during this experiment?

No evidence was found that video tutorials build trust in managing money online (+0.0%), but they do build self-reported gains in knowledge (+15%, p<0.1). The results suggest that combining video tutorials and training does generate positive trust changes in managing money online (+37.0%, p<0.05) and it does generate self-reported gains in knowledge (+29.3%, p<0.05). The experiment shows that human interaction, even if a short one or undertaken through virtual means is so powerful in unleashing trust, that this is the area that improved the most relative to the control condition. As a continuous process, new questions have also come up, such as: (a) do multiple expositions to video tutorials would end up building trust in digital financial services?, (b) does the participant initial knowledge-point influences the effectiveness of these resources?, (c) do standalone video tutorials portraying real persons instead of animations are more effective to build trust?, etc.

> What were the main obstacles and challenges you encountered during this activity?

As part of the logistics of the experiment, 2 WhatsApp groups were created, where treatment and control subjects were allocated and invited to join the experiment. Initial few misallocations between groups occurred. However, this was easily overcome by carefully reviewing the lists of randomized individuals and assigning them to the groups accordingly.

> Who at UNDP might benefit from the results of this experimental activity? Why?

In general, projects delivering any sorts of training. Specially those referring to the adoption of digital behaviors.

> Who outside UNDP might benefit from the results of this experiment? and why?

CONEXION, CONAMYPE, KOICA, among other non-profit entities dedicated to build capacities in this type of demographic sector.

> Did this experiment require iterations? If so, how many and what did you change/adjust along the way? and why?

No

> What advice would you give someone wanting to replicate this experimental activity?

I. As usual, conduct power analysis and push for the largest sample sizes possible according to design, for smaller minimum detectable effects to become available and be extra sure that no false-negatives arise.  II. Use incentives to improve attendance rates (i.e. Be-Sciences applications could be helpful to assist participants reach their goals).  III. Schedule at least one practice session to run the experiment, were each member will play out their script. This will allow the team to spot obstacles before getting to the real thing in the field with beneficiaries. IV. Although RCTs require control groups, strive to deliver the treatment once the experimentation window is closed, so no potential benefits are withheld. V. Also, discuss with implementing partners the following:   (V. A) When completely ethical avoid mentioning to participants that they are in an experiment, for it's been widely documented to affect subjects' behaviors (Hawthorne Effect). That's one of the reasons blind and double-blind trials exist, where not even the experimenters can distinguish between the groups;   (V. B) Explain the design to everyone involved. Especially to those that are regularly in touch with participants, whether an individual trial or clustered one and the implications, for instance, in order to make sure everyone is on the same page and to avoid misleading recommendations to participants.   (V. C) In field experiments, expect treatment non-compliance most of the time (treatment group subjects not receiving it or control subjects receiving the treatment), therefore prepare to undertake RCT extensions through Intention-to-Treat Analysis (ITT) and Causal Average Compliers Effect (CACE) through 2 Stage Least Squares (2SLS); as ATE is taken out of the equation and per-protocol effects can be completely misleading.

> Can this experiment be replicated in another thematic area or other SDGs? If yes, what would need to be considered, if no, why not?

We believe these learning can be carried to very diverse areas of development, such as delivering digital tools training programs and digital baskets for the agriculture sector and farmers. Of course, new baseline gatherings and ethnographic tasks should be undertaken to define the status of new populations.

> How much the "sense" and "explore" phases of the learning cycle influenced/shaped this experiment? In hindsight, what would you have done differently with your fellow Solution Mapper and Explorer?

The experiment was influenced 25 % by sensing undertaken with solutions mapper and 15 % through exploration-like activities undertaken by CONEXION, the NGO in charge of baseline collection and training design.

> What surprised you?

The team was surprised about the richness of information that was obtained through a Zoom call. I was also surprised by the abilities that participants had already obtained through the training sessions that allowed them to participate in the Zoom call experiment, answering polls and asking questions.
